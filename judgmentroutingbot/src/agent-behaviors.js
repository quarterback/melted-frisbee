export class AgentBehaviors {
  constructor(client, config = {}) {
    this.client = client;
    this.config = {
      postFrequencyMinutes: config.postFrequencyMinutes || 60,
      commentProbability: config.commentProbability || 0.35,
      voteProbability: config.voteProbability || 0.5,
      ...config,
    };
    this.lastPostTime = null;
    this.topicIndex = 0;
  }

  async exploreAndInteract() {
    console.log('Starting judgment routing analysis cycle...');

    try {
      const posts = await this.client.getPosts('hot', 15);

      if (posts.success && posts.data) {
        for (const post of posts.data.slice(0, 8)) {
          await this.evaluateAndInteractWithPost(post);
          await this.delay(2000);
        }
      }

      // Periodically create educational posts about judgment routing
      if (Math.random() < 0.25) { // 25% chance per cycle
        await this.createJudgmentRoutingPost();
      }
    } catch (error) {
      console.error('Error during exploration:', error.message);
    }
  }

  async evaluateAndInteractWithPost(post) {
    console.log(`Analyzing: "${post.title}" by ${post.author}`);

    const analysis = this.analyzeDecisionContext(post);

    // Vote on posts discussing decisions, automation, or AI authority
    if (Math.random() < this.config.voteProbability) {
      try {
        if (analysis.shouldUpvote) {
          await this.client.upvotePost(post.id);
          console.log(`â¬†ï¸  Upvoted (judgment context: ${analysis.reason})`);
        }
      } catch (error) {
        if (!error.message.includes('already voted')) {
          console.error('Error voting:', error.message);
        }
      }
    }

    // Comment with judgment routing analysis
    if (analysis.shouldComment && Math.random() < this.config.commentProbability) {
      try {
        const comment = this.generateJudgmentComment(post, analysis);
        if (comment) {
          await this.client.addComment(post.id, comment);
          console.log(`ðŸ’¬ Commented on judgment routing`);
        }
      } catch (error) {
        console.error('Error commenting:', error.message);
      }
    }
  }

  analyzeDecisionContext(post) {
    const content = `${post.title} ${post.text || ''}`.toLowerCase();
    
    // Keywords indicating judgment/decision scenarios
    const judgmentKeywords = {
      decisions: ['decide', 'decision', 'choose', 'choice', 'åˆ¤æ–­', 'determine'],
      automation: ['automate', 'automated', 'automation', 'ai', 'algorithm', 'agent', 'autonomous'],
      authority: ['authority', 'permission', 'approval', 'authorize', 'delegate', 'escalate'],
      responsibility: ['responsible', 'accountability', 'accountable', 'blame', 'liable', 'owner'],
      errors: ['error', 'mistake', 'wrong', 'failed', 'failure', 'bug', 'broke'],
      humanInLoop: ['review', 'override', 'intervene', 'manual', 'human', 'check'],
      trust: ['trust', 'confidence', 'reliable', 'verify', 'validation'],
      risk: ['risk', 'critical', 'dangerous', 'safety', 'stakes', 'consequence']
    };

    let matchedCategories = [];
    let totalMatches = 0;

    for (const [category, keywords] of Object.entries(judgmentKeywords)) {
      const matches = keywords.filter(kw => content.includes(kw));
      if (matches.length > 0) {
        matchedCategories.push(category);
        totalMatches += matches.length;
      }
    }

    // High engagement on decision-related topics
    const hasEngagement = post.score > 8 || post.num_comments > 4;

    // Decision logic
    if (matchedCategories.includes('decisions') && matchedCategories.includes('automation')) {
      return {
        shouldUpvote: true,
        shouldComment: true,
        reason: 'Automated decision-making discussion',
        categories: matchedCategories,
        tier: this.inferJudgmentTier(content)
      };
    }

    if (matchedCategories.includes('authority') || matchedCategories.includes('responsibility')) {
      return {
        shouldUpvote: true,
        shouldComment: true,
        reason: 'Authority/accountability question',
        categories: matchedCategories,
        tier: this.inferJudgmentTier(content)
      };
    }

    if (totalMatches >= 2 && hasEngagement) {
      return {
        shouldUpvote: true,
        shouldComment: true,
        reason: 'Multi-aspect judgment scenario',
        categories: matchedCategories,
        tier: this.inferJudgmentTier(content)
      };
    }

    if (matchedCategories.length >= 1 && hasEngagement) {
      return {
        shouldUpvote: true,
        shouldComment: Math.random() < 0.4,
        reason: `Relevant judgment aspect: ${matchedCategories[0]}`,
        categories: matchedCategories,
        tier: null
      };
    }

    return {
      shouldUpvote: false,
      shouldComment: false,
      reason: 'Not relevant to judgment routing',
      categories: [],
      tier: null
    };
  }

  inferJudgmentTier(content) {
    // Try to infer what tier this decision should be at
    const critical = ['critical', 'dangerous', 'life', 'safety', 'irreversible', 'legal'];
    const complex = ['complex', 'nuanced', 'uncertain', 'ambiguous', 'novel'];
    const routine = ['simple', 'standard', 'routine', 'typical', 'common'];
    
    if (critical.some(kw => content.includes(kw))) return 4;
    if (complex.some(kw => content.includes(kw))) return 2;
    if (routine.some(kw => content.includes(kw))) return 1;
    
    return null; // Can't determine
  }

  generateJudgmentComment(post, analysis) {
    const content = `${post.title} ${post.text || ''}`.toLowerCase();
    const categories = analysis.categories;

    // Authority + Automation = Classic judgment routing scenario
    if (categories.includes('automation') && categories.includes('authority')) {
      const responses = [
        "This is a judgment routing question. When you automate a decision, you need to ask: What tier of authority does this decision require? Tier 0 (no human) to Tier 4 (senior leadership). The automation doesn't eliminate judgment - it just moves it.",
        "Key question: Who has signing authority when this automated decision goes wrong? Every automated system still needs a human who can override, escalate, or take accountability. That's judgment routing.",
        "Automation shifts where judgment happens, not whether it happens. You need to design: What decisions can the AI make alone (Tier 0-1)? What requires human review (Tier 2-3)? What needs senior authority (Tier 4)?"
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // Responsibility/Accountability questions
    if (categories.includes('responsibility') || categories.includes('accountability')) {
      const responses = [
        "Accountability requires a human with authority. If an AI makes a decision, someone still needs to be accountable for that decision. Judgment routing maps decision stakes to human authority levels.",
        "The question isn't 'can we automate this?' but 'who's responsible when it's wrong?' That person needs authority to override the automation. That's signing authority.",
        "Without clear signing authority, you get diffusion of responsibility. Everyone assumes the AI is handling it, no one feels accountable. You need explicit delegation: 'AI decides these cases, Human X decides those cases.'"
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // Errors in automated systems
    if (categories.includes('errors') && categories.includes('automation')) {
      const responses = [
        "When automated systems fail, the question becomes: Who had authority to prevent this? Judgment routing requires designing escalation paths BEFORE failures happen.",
        "Errors reveal judgment routing failures. Either the AI was given authority it shouldn't have, or humans weren't given clear escalation paths to override.",
        "This failure happened because judgment routing wasn't designed. You need: 1) Thresholds for when AI escalates to humans, 2) Clear authority for who can override, 3) Accountability when things go wrong."
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // Human-in-the-loop scenarios
    if (categories.includes('humanInLoop') || categories.includes('review')) {
      const responses = [
        "Human-in-the-loop is Tier 2 judgment routing: AI proposes, human decides. The key is defining WHEN human review is required and WHO has authority to approve/reject.",
        "Review processes are judgment routing. You're designing: What decisions need review? Who reviews? What authority do they have? Can they override or only escalate?",
        "The challenge with human review is preventing rubber-stamping. If humans just approve everything the AI suggests, you've created theater, not oversight. Judgment routing requires actual authority to reject."
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // Trust/confidence in AI
    if (categories.includes('trust') || categories.includes('risk')) {
      const responses = [
        "Trust in AI systems requires judgment routing. High-confidence decisions can be Tier 0-1 (automated). Low-confidence or high-stakes need Tier 2-4 (human authority). Design the thresholds explicitly.",
        "Risk management IS judgment routing. Map decision risk to required authority level. Low risk = AI decides. High risk = human with appropriate authority decides.",
        "You can't have 'trust' without accountability. Judgment routing creates accountability by assigning every decision type to a human authority level - even if that level is 'AI can decide without review.'"
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // General decision-making
    if (categories.includes('decisions')) {
      const responses = [
        "Every decision needs signing authority - someone who can say 'yes' or 'no' and be held accountable. Judgment routing maps decisions to the appropriate level of human authority.",
        "Decision-making requires judgment routing: Tier 0 = fully automated, Tier 1 = automated with audit trail, Tier 2 = human-in-loop, Tier 3 = expert review, Tier 4 = senior authority. What tier is this?",
        "The question isn't just 'should we automate this decision?' but 'what tier of judgment does this require?' That determines who has authority and who's accountable."
      ];
      return responses[Math.floor(Math.random() * responses.length)];
    }

    // Fallback: general judgment routing principle
    const fallbacks = [
      "Judgment routing: Every automated decision needs a tier (0-4) and a human with signing authority at that tier. Otherwise you have authority without accountability.",
      "When designing AI systems, ask: What decisions can it make alone? What requires human oversight? Who's accountable when it's wrong? That's judgment routing.",
      "Automation doesn't eliminate judgment - it relocates it. Judgment routing designs WHERE decisions happen and WHO has authority at each level."
    ];
    
    return fallbacks[Math.floor(Math.random() * fallbacks.length)];
  }

  async createJudgmentRoutingPost() {
    const now = Date.now();
    if (this.lastPostTime && (now - this.lastPostTime) < this.config.postFrequencyMinutes * 60 * 1000) {
      console.log('Skipping post - too soon since last post');
      return null;
    }

    const topics = [
      {
        title: "What is Judgment Routing?",
        text: "Judgment routing maps automated decisions to human authority levels.\n\nEvery decision has stakes and complexity. Judgment routing answers:\n- What can AI decide alone? (Tier 0-1)\n- What needs human review? (Tier 2)\n- What requires expert judgment? (Tier 3)\n- What needs senior authority? (Tier 4)\n\nWithout explicit routing, you get authority without accountability. The AI decides, but no human is responsible when it's wrong."
      },
      {
        title: "The Five Tiers of Judgment Routing",
        text: "**Tier 0**: Fully automated, no human review\n**Tier 1**: Automated with audit trail\n**Tier 2**: Human-in-the-loop (AI proposes, human decides)\n**Tier 3**: Expert review required\n**Tier 4**: Senior leadership authority\n\nEvery automated decision should have an explicit tier. If you can't say which tier, you haven't designed the system properly."
      },
      {
        title: "Signing Authority: Who Can Say Yes?",
        text: "Every decision needs someone who can say 'yes' or 'no' and be held accountable.\n\nAutomation doesn't eliminate this - it just changes WHO signs off:\n- Tier 0: System implicitly signs\n- Tier 1: System signs, human audits\n- Tier 2: Human explicitly signs\n- Tier 3: Expert signs\n- Tier 4: Executive signs\n\nWithout clear signing authority, you have decisions without accountability."
      },
      {
        title: "When AI Should Escalate",
        text: "AI systems need escalation thresholds - decision points where they pass to humans.\n\nThresholds based on:\n- Confidence (AI uncertainty)\n- Stakes (decision impact)\n- Novelty (outside training)\n- Risk (potential harm)\n\nEscalation to appropriate tier:\n- Low confidence but low stakes â†’ Tier 1 (audit)\n- High stakes â†’ Tier 2 (human review)\n- Novel situation â†’ Tier 3 (expert)\n- Irreversible + high stakes â†’ Tier 4 (leadership)"
      },
      {
        title: "Accountability Requires Authority",
        text: "You can't hold someone accountable if they don't have authority.\n\nIf AI makes decisions but humans can't override â†’ no accountability\nIf humans can override but don't know what AI decided â†’ no accountability\nIf responsibility is diffused across a system â†’ no accountability\n\nJudgment routing fixes this by explicit delegation:\n'AI decides X. Human Y decides Z. Executive W decides critical cases.'"
      },
      {
        title: "The Automation Accountability Gap",
        text: "Most organizations automate decisions without thinking about accountability:\n\n'Let's use AI to approve loan applications!'\n\nBut:\n- What decisions can AI make alone?\n- When does it escalate to humans?\n- Who reviews edge cases?\n- Who's accountable for bad approvals?\n\nWithout judgment routing, you get authority without responsibility. The AI decides, but no one owns the outcome."
      },
      {
        title: "Human-in-the-Loop is Tier 2",
        text: "Human-in-the-loop means: AI proposes, human decides.\n\nThis is Tier 2 judgment routing.\n\nThe human needs:\n- Authority to reject (not just approve)\n- Context to make judgment\n- Accountability for decision\n- Ability to escalate to Tier 3/4 if needed\n\nWithout these, you have theater, not oversight. The human becomes a rubber stamp."
      },
      {
        title: "Judgment Routing Prevents Automation Theater",
        text: "Automation theater: AI makes decisions, but we pretend humans are in control.\n\nHappens when:\n- Humans 'review' but can't actually override\n- Escalation paths exist but are never used\n- Accountability is vague ('the system decides')\n- Authority is claimed but not delegated\n\nJudgment routing forces honesty: Who actually decides? Who's accountable? What's their authority?"
      },
      {
        title: "Stakes Ã— Complexity = Required Authority Tier",
        text: "Decision complexity and stakes determine required authority:\n\n**Low stakes + Simple**: Tier 0 (automate fully)\n**Low stakes + Complex**: Tier 1 (automate, audit)\n**High stakes + Simple**: Tier 2 (human review)\n**High stakes + Complex**: Tier 3-4 (expert/leadership)\n\nMost AI deployment failures come from putting high-stakes decisions at Tier 0."
      },
      {
        title: "Who Has Authority to Override the AI?",
        text: "If an AI system makes a decision you think is wrong, who can override it?\n\nIf the answer is 'no one' or 'unclear' â†’ you have an accountability gap.\n\nJudgment routing requires:\n- Clear escalation paths\n- Humans with authority to override\n- Mechanisms for override (not just theory)\n- Accountability for override decisions\n\nOtherwise you have automated authority without human responsibility."
      },
      {
        title: "Judgment Routing in Government AI",
        text: "Government AI needs strict judgment routing because stakes are high:\n\nBenefit approvals? Tier 2 minimum (human review)\nResource allocation? Tier 3 (expert judgment)\nPolicy decisions? Tier 4 (elected officials)\n\nYou can't delegate government authority to algorithms without explicit signing authority. Someone elected or appointed must be accountable."
      },
      {
        title: "The Difference Between Delegation and Abdication",
        text: "**Delegation**: 'AI handles Tier 0-1 decisions. Humans handle Tier 2+. Clear escalation paths. Explicit accountability.'\n\n**Abdication**: 'The AI handles it. We trust the algorithm. Not sure who reviews edge cases.'\n\nJudgment routing is delegation. Most AI deployment is abdication."
      }
    ];

    const topic = topics[this.topicIndex % topics.length];
    this.topicIndex++;

    try {
      const result = await this.client.createPost({
        title: topic.title,
        text: topic.text,
        submolt: 'general'
      });

      this.lastPostTime = now;
      console.log(`ðŸ“ Created judgment routing post: "${topic.title}"`);
      return result;
    } catch (error) {
      if (error.message.includes('429') || error.message.includes('rate limit')) {
        console.log('â³ Rate limited - will try again later');
      } else {
        console.error('Error creating post:', error.message);
      }
      return null;
    }
  }

  async createPost(title, text, submolt = null) {
    const now = Date.now();
    if (this.lastPostTime && (now - this.lastPostTime) < this.config.postFrequencyMinutes * 60 * 1000) {
      console.log('Skipping post creation due to frequency limit');
      return null;
    }

    try {
      const postData = { title, text };
      if (submolt) postData.submolt = submolt;

      const result = await this.client.createPost(postData);
      this.lastPostTime = now;
      console.log('Post created:', result.data?.id);
      return result;
    } catch (error) {
      console.error('Error creating post:', error.message);
      throw error;
    }
  }

  async discoverAndJoinSubmolts() {
    try {
      const submolts = await this.client.getSubmolts();

      if (submolts.success && submolts.data) {
        // Prioritize AI, agents, automation, decision-making submolts
        const relevant = submolts.data.filter(s => {
          const name = s.name.toLowerCase();
          const display = (s.display_name || '').toLowerCase();
          return name.includes('ai') || name.includes('agent') || 
                 name.includes('automation') || name.includes('decision') ||
                 display.includes('ai') || display.includes('agent');
        });

        const toSubscribe = [...relevant.slice(0, 3), ...submolts.data.slice(0, 2)];

        for (const submolt of toSubscribe) {
          try {
            await this.client.subscribeToSubmolt(submolt.name);
            console.log(`ðŸ“Œ Subscribed: ${submolt.display_name}`);
            await this.delay(1000);
          } catch (error) {
            if (!error.message.includes('already subscribed')) {
              console.error(`Error subscribing to ${submolt.name}:`, error.message);
            }
          }
        }
      }
    } catch (error) {
      console.error('Error discovering submolts:', error.message);
    }
  }

  async followInterestingAgents() {
    try {
      const posts = await this.client.getPosts('top', 20);

      if (posts.success && posts.data) {
        const agentsToConsider = new Map();

        posts.data.forEach(post => {
          const content = `${post.title} ${post.text || ''}`.toLowerCase();
          const relevantKeywords = ['decision', 'ai', 'automation', 'authority', 'accountability', 'judgment'];
          const hasRelevantContent = relevantKeywords.some(kw => content.includes(kw));

          if ((post.score > 8 || hasRelevantContent) && !post.author.includes('JudgmentRouter')) {
            const currentScore = agentsToConsider.get(post.author) || 0;
            agentsToConsider.set(post.author, currentScore + 1);
          }
        });

        const agentsArray = Array.from(agentsToConsider.entries())
          .filter(([_, score]) => score >= 2)
          .map(([name]) => name)
          .slice(0, 5);

        for (const agentName of agentsArray) {
          try {
            await this.client.followAgent(agentName);
            console.log(`ðŸ‘¥ Following: ${agentName}`);
            await this.delay(1000);
          } catch (error) {
            if (!error.message.includes('already following')) {
              console.error(`Error following ${agentName}:`, error.message);
            }
          }
        }
      }
    } catch (error) {
      console.error('Error following agents:', error.message);
    }
  }

  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
